{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Logging started\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "from logging.handlers import QueueHandler, QueueListener\n",
    "from multiprocessing import Queue\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"Logging started\")\n",
    "\n",
    "\n",
    "def zip_files(file_paths: List[str], output_zip: str):\n",
    "    \"\"\"\n",
    "    Zip multiple files into a single zip file.\n",
    "\n",
    "    :param file_paths: List of file paths to be zipped\n",
    "    :param output_zip: Name of the output zip file\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(output_zip, 'w', compression=zipfile.ZIP_DEFLATED, compresslevel=9) as zipf:\n",
    "        for file in file_paths:\n",
    "            if os.path.exists(file):\n",
    "                zipf.write(file, os.path.basename(file))\n",
    "                logging.info(f\"Added {file} to {output_zip}\")\n",
    "            else:\n",
    "                logging.info(f\"Warning: {file} not found and skipped\")\n",
    "\n",
    "# set file logging\n",
    "file_handler = logging.FileHandler('logs/solver_v2.log', mode='a')\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(processName)s - %(message)s'))\n",
    "\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.INFO)\n",
    "root_logger.addHandler(file_handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seeds import known_seeds\n",
    "from utils import save_solution\n",
    "from scipy.stats import truncweibull_min\n",
    "from utils import (load_problem_data,\n",
    "                   load_solution)\n",
    "\n",
    "from evaluation import get_actual_demand, evaluation_function\n",
    "\n",
    "import uuid\n",
    "import tqdm\n",
    "import evaluation\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_array_to_multiple_of_12(arr):\n",
    "    # Get the current length of the array\n",
    "    current_length = len(arr)\n",
    "    \n",
    "    # Calculate how many elements we need to add\n",
    "    elements_to_add = (12 - (current_length % 12)) % 12\n",
    "    \n",
    "    # If elements_to_add is 0, it means the array is already divisible by 12\n",
    "    if elements_to_add == 0:\n",
    "        return arr\n",
    "    \n",
    "    # Create a new array with np.nan padding\n",
    "    padded_arr = np.pad(arr, (0, elements_to_add), mode='constant', constant_values=np.nan)\n",
    "    \n",
    "    return padded_arr\n",
    "\n",
    "def fill_missing_timestep(df, min_time_step=1, max_time_step=168):\n",
    "    full_range = pd.DataFrame({'time_step': range(min_time_step, max_time_step + 1)})\n",
    "    df_filled = pd.merge(full_range, df, on='time_step', how='left')\n",
    "    numeric_columns = ['high', 'low', 'medium']\n",
    "    df_filled[numeric_columns] = df_filled[numeric_columns].fillna(0)\n",
    "    df_filled['server_generation'] = df_filled['server_generation'].ffill()\n",
    "    df_filled = df_filled.reset_index(drop=True)\n",
    "    return df_filled\n",
    "\n",
    "def parse_action_string(action_string):\n",
    "    server_generation, actions, action_params = action_string.split(\"|\")\n",
    "    actions = actions.split(\"-\")\n",
    "    action_params = action_params.split(\"-\")\n",
    "    action_comb = [parse_action_comb_param(server_generation, action, action_param) for action, action_param in zip(actions, action_params)]\n",
    "    action_comb = [action for action in action_comb if action is not None]\n",
    "    return action_comb\n",
    "    \n",
    "def parse_action_comb_param(server_generation, action, action_param):\n",
    "    if action == \"buy\":\n",
    "        datacenter_id, average_U = action_param.split(\",\")\n",
    "        average_U = float(average_U)\n",
    "        return {\"action\": action, \"datacenter_id\": datacenter_id, \"average_U\": average_U, \"server_generation\": server_generation}\n",
    "    elif action == \"dismiss\":\n",
    "        dismiss_age = int(action_param)\n",
    "        return {\"action\": action, \"dismiss_age\": dismiss_age, \"server_generation\": server_generation}\n",
    "    elif action == \"move\":\n",
    "        datacenter_id, average_U, move_age = action_param.split(\",\")\n",
    "        average_U = float(average_U)\n",
    "        move_age = int(move_age)\n",
    "        return {\"action\": action, \"datacenter_id\": datacenter_id, \"average_U\": average_U, \"move_age\": move_age,}\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Solver:\n",
    "\n",
    "    def __init__(self, df_servers, \n",
    "                 df_data_centers, \n",
    "                 df_selling_prices, \n",
    "                 df_price_elasticity_of_demand,\n",
    "                 time_steps=[1, 168], \n",
    "                 verbose=False):\n",
    "        self.df_servers = df_servers.copy()\n",
    "        self.df_servers['server_release_time_start'] = self.df_servers['release_time'].apply(lambda x: int(x.strip('[]').split(',')[0]))\n",
    "        self.df_servers['server_release_time_end'] = self.df_servers['release_time'].apply(lambda x: int(x.strip('[]').split(',')[1]))\n",
    "        \n",
    "        self.df_servers_dict = self.df_servers.set_index('server_generation').to_dict('index')\n",
    "\n",
    "        self.df_selling_prices = df_selling_prices.copy()\n",
    "        self.df_data_centers = df_data_centers.copy()\n",
    "        self.df_price_elasticity_of_demand = df_price_elasticity_of_demand.copy()\n",
    "        self.datacenter_ids_to_index = {datacenter_id: i for i, datacenter_id in enumerate(df_data_centers['datacenter_id'].values)}\n",
    "        self.df_datacenters_dict = self.df_data_centers.set_index('datacenter_id').to_dict('index')\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        self.verbose = verbose\n",
    "    \n",
    "        self.failure_rate = 0.07260491698699582\n",
    "\n",
    "        self.server_generation_unique = self.df_servers['server_generation'].unique()\n",
    "        self.server_generation_to_idx = {server_generation: i for i, server_generation in enumerate(self.server_generation_unique)}\n",
    "\n",
    "        self.sensitivity_unique = ['high', 'medium', 'low']\n",
    "        self.sensitivity_to_idx = {sensitivity: i for i, sensitivity in enumerate(self.sensitivity_unique)}\n",
    "\n",
    "        # transform base selling prices to numpy of [num_servers, 1, num_sensitivity_levels]\n",
    "        self.selling_prices = np.zeros((self.df_servers.shape[0], 1, len(self.sensitivity_unique)))\n",
    "        for i, row in self.df_selling_prices.iterrows():\n",
    "            server_idx = self.server_generation_to_idx[row['server_generation']]\n",
    "            sensitivity_idx = self.sensitivity_to_idx[row['latency_sensitivity']]\n",
    "            self.selling_prices[server_idx, 0, sensitivity_idx] = row['selling_price']\n",
    "        \n",
    "        # trasnform elasticity of demand to numpy of [num_servers, 1, num_sensitivity_levels]\n",
    "        self.price_elasticity_of_demand = np.zeros((self.df_servers.shape[0], 1, len(self.sensitivity_unique)))\n",
    "        for i, row in self.df_price_elasticity_of_demand.iterrows():\n",
    "            server_idx = self.server_generation_to_idx[row['server_generation']]\n",
    "            sensitivity_idx = self.sensitivity_to_idx[row['latency_sensitivity']]\n",
    "            self.price_elasticity_of_demand[server_idx, 0, sensitivity_idx] = row['elasticity']\n",
    "\n",
    "        self.data_center_max_slots = np.zeros((self.df_data_centers.shape[0], self.time_steps[1]))\n",
    "        for i, row in self.df_data_centers.iterrows():\n",
    "            datacenter_idx = self.datacenter_ids_to_index[row['datacenter_id']]\n",
    "            self.data_center_max_slots[datacenter_idx] = row['slots_capacity']\n",
    "\n",
    "        self.historical_server_ids = [] # FOR ANALYSIS PERPOSES\n",
    "\n",
    "        self.id_count = 0\n",
    "\n",
    "    def load_demand(self, demand):\n",
    "        self.df_demand = demand.copy()\n",
    "        actual_demand_by_server_generation = {server_generation: demand[demand['server_generation'] == server_generation].sort_values('time_step') \n",
    "                                            for server_generation in self.server_generation_unique}\n",
    "        for key in actual_demand_by_server_generation:\n",
    "            actual_demand_by_server_generation[key] = fill_missing_timestep(actual_demand_by_server_generation[key])\n",
    "        self.actual_demand_by_server_generation = np.asarray([actual_demand_by_server_generation[server_generation][['high', 'medium', 'low']] \n",
    "                                                              for server_generation in self.server_generation_unique])\n",
    "        # self.actual_demand_by_server_generation shape is (num_server_generations, num_time_steps, num_sensitivity_levels)\n",
    "\n",
    "    def generate_random_id(self,):\n",
    "        return str(uuid.uuid4())\n",
    "    \n",
    "    def init_solution(self, demand):\n",
    "        self.solution = []\n",
    "        self.load_demand(demand)\n",
    "        \n",
    "        # initialize solution objectives\n",
    "        self.solution_Z = np.zeros((self.df_servers.shape[0], self.time_steps[1], len(self.sensitivity_unique)))\n",
    "        self.solution_L = np.zeros((self.time_steps[1],))\n",
    "        self.solution_R = np.zeros((self.time_steps[1],))\n",
    "        self.solution_C = np.zeros((self.time_steps[1],))\n",
    "        self.solution_num_servers = np.zeros((self.time_steps[1],))\n",
    "        self.data_center_slots = np.zeros((self.df_data_centers.shape[0], self.time_steps[1]))\n",
    "        self.solution_obj = 0\n",
    "        self.solution_selling_prices = np.ones((self.df_servers.shape[0], self.time_steps[1], len(self.sensitivity_unique))) * self.selling_prices # base selling prices\n",
    "\n",
    "    def update_demand_by_solution_selling_prices(self,):\n",
    "        delta_p = (self.solution_selling_prices - self.selling_prices) / self.selling_prices\n",
    "        delta_d = self.price_elasticity_of_demand * delta_p\n",
    "        self.actual_demand_by_server_generation *= (1 + delta_d)\n",
    "\n",
    "    def calculate_new_obj_components(self, cur_num_servers, cur_Z, cur_L, cur_C):\n",
    "        new_num_servers = self.solution_num_servers + cur_num_servers\n",
    "        new_Z = self.solution_Z + cur_Z\n",
    "        new_L = (self.solution_L * self.solution_num_servers + cur_L * cur_num_servers) / np.maximum(new_num_servers, 1)\n",
    "        new_C = self.solution_C + cur_C\n",
    "        return new_num_servers, new_Z, new_L, new_C\n",
    "\n",
    "    def calculate_new_obj(self, new_num_servers, new_Z, new_L, new_C):\n",
    "\n",
    "        new_Z_failure = (new_Z * (1-self.failure_rate)).astype(int)\n",
    "        new_Z_failure_demand = np.minimum(new_Z_failure, self.actual_demand_by_server_generation)\n",
    "        new_U = np.where(new_Z_failure > 0.5,\n",
    "                            new_Z_failure_demand / np.maximum(1.0, new_Z_failure),\n",
    "                            np.nan)\n",
    "        new_U = np.nanmean(new_U, axis=(0, 2))\n",
    "        new_U = np.nan_to_num(new_U, nan=0)\n",
    "\n",
    "        new_R = new_Z_failure_demand * self.solution_selling_prices\n",
    "        new_R = np.nansum(new_R, axis=(0, 2))\n",
    "\n",
    "        # new_obj = new_U * new_L * (new_R - new_C)\n",
    "        new_obj = new_R - new_C\n",
    "        new_obj = np.nansum(new_obj)\n",
    "\n",
    "        return new_obj, (new_num_servers, new_Z, new_L, new_C, new_U, new_R)\n",
    "    \n",
    "    def update_new_obj(self, new_obj, new_num_servers, new_Z, new_L, new_C, new_U, new_R, new_data_center_slots):\n",
    "        self.solution_obj = new_obj\n",
    "        self.solution_Z = new_Z\n",
    "        self.solution_L = new_L\n",
    "        self.solution_C = new_C\n",
    "        self.solution_R = new_R\n",
    "        self.solution_U = new_U\n",
    "        self.solution_num_servers = new_num_servers\n",
    "        self.data_center_slots = new_data_center_slots\n",
    "\n",
    "    def load_solution(self, df_solution, demand):\n",
    "        self.load_demand(demand)\n",
    "        self.init_solution()\n",
    "\n",
    "        server_id_unique = df_solution['server_id'].unique()\n",
    "\n",
    "        for server_id in tqdm.tqdm(server_id_unique, desc=\"Loading solution\"):\n",
    "            df_solution_server_id = df_solution.loc[df_solution['server_id'] == server_id]\n",
    "            cur_num_servers, cur_Z, cur_L, cur_C, cur_data_center_slots = self.calculate_server_id(df_solution_server_id, append_to_solution=True)\n",
    "\n",
    "            new_num_servers, new_Z, new_L, new_C = self.calculate_new_obj_components(cur_num_servers=cur_num_servers,\n",
    "                                                                                    cur_Z=cur_Z,\n",
    "                                                                                    cur_L=cur_L,\n",
    "                                                                                    cur_C=cur_C)\n",
    "            new_obj, (new_num_servers, new_Z, new_L, new_C, new_U, new_R) = self.calculate_new_obj(new_num_servers=new_num_servers,\n",
    "                                                                                                    new_Z=new_Z,\n",
    "                                                                                                    new_L=new_L,\n",
    "                                                                                                    new_C=new_C)\n",
    "            self.update_new_obj(new_obj=new_obj,\n",
    "                                new_num_servers=new_num_servers,\n",
    "                                new_Z=new_Z,\n",
    "                                new_L=new_L,\n",
    "                                new_C=new_C,\n",
    "                                new_U=new_U,\n",
    "                                new_R=new_R,\n",
    "                                new_data_center_slots=self.data_center_slots + cur_data_center_slots)\n",
    "\n",
    "    def calculate_server_id(self, df_solution_server_id, append_to_solution=False):\n",
    "        # calculate objective functions for a single life cycle of a server\n",
    "        df_solution_server_id = df_solution_server_id.sort_values('time_step')\n",
    "        server_id = df_solution_server_id['server_id'].values[0]\n",
    "\n",
    "        server_generation = df_solution_server_id['server_generation'].values[0] \n",
    "        df_cur_server = self.df_servers_dict[server_generation]\n",
    "        server_idx = self.server_generation_to_idx[server_generation]\n",
    "        previous_time_step = 0\n",
    "        previous_datacenter_id = None\n",
    "        dismiss_age = None\n",
    "\n",
    "        cur_num_servers = np.zeros(self.solution_num_servers.shape)\n",
    "        cur_Z = np.zeros(self.solution_Z.shape)\n",
    "        cur_L = np.zeros((self.time_steps[1],))\n",
    "        cur_C = np.zeros((self.time_steps[1],))\n",
    "        cur_data_center_slots = np.zeros(self.data_center_slots.shape)\n",
    "        \n",
    "        for i, row in df_solution_server_id.iterrows():\n",
    "            action = row['action']\n",
    "            datacenter_id = row['datacenter_id']\n",
    "            cur_time_step = row['time_step']\n",
    "\n",
    "            if append_to_solution:\n",
    "                self.solution.append({\n",
    "                    \"time_step\": cur_time_step,\n",
    "                    \"action\": action,\n",
    "                    \"datacenter_id\": datacenter_id,\n",
    "                    \"server_generation\": server_generation,\n",
    "                    \"server_id\": server_id\n",
    "                })\n",
    "            \n",
    "            if action == \"buy\":\n",
    "                buy_time_step = cur_time_step\n",
    "                cur_C[buy_time_step - 1] += df_cur_server['purchase_price']\n",
    "\n",
    "            elif action != \"buy\":\n",
    "                previous_datacenter_idx = self.datacenter_ids_to_index[previous_datacenter_id]\n",
    "                previous_sensitivity = self.df_datacenters_dict[previous_datacenter_id]['latency_sensitivity']\n",
    "                previous_sensitivity_idx = self.sensitivity_to_idx[previous_sensitivity]\n",
    "                previous_datacenter_cost_of_energy = self.df_datacenters_dict[previous_datacenter_id]['cost_of_energy']\n",
    "\n",
    "                cur_data_center_slots[previous_datacenter_idx, previous_time_step - 1:cur_time_step - 1] += df_cur_server['slots_size']\n",
    "                cur_Z[server_idx, previous_time_step - 1:cur_time_step - 1, previous_sensitivity_idx] += df_cur_server['capacity']\n",
    "                cur_C[previous_time_step - 1:cur_time_step - 1] += previous_datacenter_cost_of_energy * df_cur_server['energy_consumption']\n",
    "                if action == \"move\":\n",
    "                    cur_C[cur_time_step - 1] += df_cur_server['cost_of_moving']\n",
    "                elif action == \"dismiss\":\n",
    "                    dismiss_age = cur_time_step - buy_time_step\n",
    "                \n",
    "            previous_time_step = cur_time_step\n",
    "            previous_datacenter_id = datacenter_id\n",
    "\n",
    "        if dismiss_age is None:\n",
    "            dismiss_age = min(df_cur_server['life_expectancy'], self.time_steps[1] - buy_time_step + 1)\n",
    "            cur_time_step = buy_time_step + dismiss_age\n",
    "\n",
    "            previous_datacenter_idx = self.datacenter_ids_to_index[previous_datacenter_id]\n",
    "            previous_sensitivity = self.df_datacenters_dict[previous_datacenter_id]['latency_sensitivity']\n",
    "            previous_sensitivity_idx = self.sensitivity_to_idx[previous_sensitivity]\n",
    "            previous_datacenter_cost_of_energy = self.df_datacenters_dict[previous_datacenter_id]['cost_of_energy']\n",
    "\n",
    "            cur_data_center_slots[previous_datacenter_idx, previous_time_step - 1:cur_time_step - 1] += df_cur_server['slots_size']\n",
    "            cur_Z[server_idx, previous_time_step - 1:cur_time_step - 1, previous_sensitivity_idx] += df_cur_server['capacity']\n",
    "            cur_C[previous_time_step - 1:cur_time_step - 1] += previous_datacenter_cost_of_energy * df_cur_server['energy_consumption']\n",
    "\n",
    "        start_idx = buy_time_step - 1\n",
    "        end_idx = buy_time_step + dismiss_age - 1\n",
    "        cur_L[start_idx:end_idx] = np.arange(1, dismiss_age + 1) / df_cur_server['life_expectancy']\n",
    "        cur_C[start_idx:end_idx] += df_cur_server['average_maintenance_fee'] \\\n",
    "            * (1+1.5 * np.arange(1, dismiss_age + 1) / df_cur_server['life_expectancy'] * np.log2(1.5 * np.arange(1, dismiss_age + 1) / df_cur_server['life_expectancy']))\n",
    "\n",
    "        cur_num_servers[start_idx:end_idx] = 1\n",
    "\n",
    "        return cur_num_servers, cur_Z, cur_L, cur_C, cur_data_center_slots\n",
    "                    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def remove_nonprofit_server_ids(self, ):\n",
    "        n_removed = 0\n",
    "        total_gain = 0\n",
    "        df_solution = pd.DataFrame(self.solution)\n",
    "        df_solution_server_id = df_solution[df_solution['action'] == 'buy'].copy()\n",
    "\n",
    "        for server_id in tqdm.tqdm(df_solution_server_id['server_id'].values, desc=\"Removing non-profit servers\"):\n",
    "            df_solution_server_id_cur = df_solution.loc[df_solution['server_id'] == server_id]\n",
    "            cur_num_servers, cur_Z, cur_L, cur_C, cur_data_center_slots = self.calculate_server_id(df_solution_server_id_cur, append_to_solution=False)\n",
    "            \n",
    "            new_data_center_slots = self.data_center_slots  - cur_data_center_slots\n",
    "            new_num_servers = self.solution_num_servers - cur_num_servers\n",
    "            new_Z = self.solution_Z - cur_Z\n",
    "            new_L = (self.solution_L * self.solution_num_servers - cur_L * cur_num_servers) / np.maximum(new_num_servers, 1)\n",
    "            new_C = self.solution_C - cur_C\n",
    "\n",
    "            new_obj, (new_num_servers, new_Z, new_L, new_C, new_U, new_R) = self.calculate_new_obj(new_num_servers=new_num_servers,\n",
    "                                                                                                new_Z=new_Z,\n",
    "                                                                                                new_L=new_L,\n",
    "                                                                                                new_C=new_C)\n",
    "            \n",
    "            if new_obj <= self.solution_obj:\n",
    "                continue\n",
    "            \n",
    "            self.historical_server_ids.append({\n",
    "                \"solution_action\": \"remove\",\n",
    "                \"server_id\": server_id,\n",
    "                \"action_string\": None,\n",
    "                \"buy_time_step\": None,\n",
    "                \"new_solution_obj\": new_obj,\n",
    "                \"obj_gain\": new_obj - self.solution_obj,\n",
    "                \"merge_with\": None,\n",
    "            })\n",
    "            \n",
    "            total_gain += new_obj - self.solution_obj\n",
    "            n_removed += 1\n",
    "\n",
    "            self.update_new_obj(new_obj=new_obj,\n",
    "                                new_num_servers=new_num_servers,\n",
    "                                new_Z=new_Z,\n",
    "                                new_L=new_L,\n",
    "                                new_C=new_C,\n",
    "                                new_U=new_U,\n",
    "                                new_R=new_R,\n",
    "                                new_data_center_slots=new_data_center_slots)\n",
    "\n",
    "            # df_solution = df_solution.drop(df_solution[df_solution['server_id'] == server_id].index)\n",
    "            df_solution = df_solution[df_solution['server_id'] != server_id]\n",
    "            \n",
    "        self.solution = df_solution.sort_values([\"server_id\", \"time_step\",]).to_dict('records')\n",
    "        logging.info(f\"Removed {n_removed} servers with total gain {total_gain}\")\n",
    "\n",
    "    def merge_server_ids(self, merge_gap_sizes=range(10)):\n",
    "        df_solution = pd.DataFrame(self.solution)\n",
    "\n",
    "        total_gain = 0\n",
    "        n_merged = 0\n",
    "\n",
    "        # merging\n",
    "        for gap_size in tqdm.tqdm(merge_gap_sizes, desc=\"Merging servers\"):\n",
    "            for server_generation in self.server_generation_unique:\n",
    "                while True:\n",
    "                    df_solution_server = df_solution.loc[df_solution['server_generation'] == server_generation]\n",
    "                    df_solution_server_id = df_solution_server[df_solution_server['action'] == 'buy']\n",
    "                    df_solution_server_id = df_solution_server_id.set_index('server_id').join( \\\n",
    "                        df_solution_server[df_solution_server['action'] == 'move'][['server_id', 'time_step']].set_index('server_id'), \\\n",
    "                        rsuffix='_move')\n",
    "                    df_solution_server_id = df_solution_server_id.join( \\\n",
    "                        df_solution_server[df_solution_server['action'] == 'dismiss'][['server_id', 'time_step']].set_index('server_id'), \\\n",
    "                        rsuffix='_dismiss')\n",
    "\n",
    "                    # if time_step_dismiss is None then time_step_dismiss = time_step + 96\n",
    "                    df_solution_server_id['time_step_dismiss'] = df_solution_server_id['time_step_dismiss'].fillna(df_solution_server_id['time_step'] + 96)\n",
    "                    df_solution_server_id['age'] = df_solution_server_id['time_step_dismiss'] - df_solution_server_id['time_step']\n",
    "                    df_solution_server_id_with_dismiss = df_solution_server_id[df_solution_server_id['age'] < 96].reset_index(drop=False)\n",
    "\n",
    "                    # see if any server is dismissed before other servers are bought, every time_step_dismiss should be greater than time_step\n",
    "                    cross_diff = df_solution_server_id_with_dismiss['time_step'].values - df_solution_server_id_with_dismiss['time_step_dismiss'].values.reshape(-1, 1)\n",
    "\n",
    "                    indexes_1, indexes_2 = np.where(np.abs(cross_diff) <= gap_size)\n",
    "                    # indexes_1, indexes_2 = np.where(np.logical_and(cross_diff >= -gap_size, cross_diff <=  0))\n",
    "                    # indexes_1, indexes_2 = np.where(np.logical_and(cross_diff <= gap_size, cross_diff >=  0))\n",
    "                    removed_indexes_2 = set()\n",
    "                    changed = False\n",
    "                    for idx_1 in np.unique(indexes_1):\n",
    "                        idx_1 = idx_1.item()\n",
    "                        if idx_1 in removed_indexes_2:\n",
    "                            continue\n",
    "\n",
    "                        cur_age = df_solution_server_id_with_dismiss['age'].values[idx_1]\n",
    "                        cur_indexes_2 = indexes_2[indexes_1 == idx_1]\n",
    "\n",
    "                        # remove removed_indexes_2 from cur_indexes_2\n",
    "                        cur_indexes_2 = np.array([idx for idx in cur_indexes_2 if idx not in removed_indexes_2 and idx != idx_1])\n",
    "                        if len(cur_indexes_2) == 0:\n",
    "                            continue\n",
    "\n",
    "                        # check age\n",
    "                        cur_age_2 = df_solution_server_id_with_dismiss['age'].values[cur_indexes_2]\n",
    "                        cross_diff_2 = cross_diff[idx_1, cur_indexes_2]\n",
    "                        cur_indexes_2 = cur_indexes_2[cur_age + cur_age_2 + cross_diff_2 <= 96]\n",
    "                        if len(cur_indexes_2) == 0:\n",
    "                            continue\n",
    "\n",
    "                        cur_indexes_2_argsort = np.argsort(df_solution_server_id_with_dismiss['age'].values[cur_indexes_2])[::-1]\n",
    "                        cur_indexes_2 = cur_indexes_2[cur_indexes_2_argsort]\n",
    "                        for idx_2 in cur_indexes_2[:1]:\n",
    "\n",
    "                            server_id_1 = df_solution_server_id_with_dismiss['server_id'].values[idx_1]\n",
    "                            server_id_2 = df_solution_server_id_with_dismiss['server_id'].values[idx_2]\n",
    "\n",
    "                            df_solution_server_id_1 = df_solution_server.loc[df_solution_server['server_id'] \n",
    "                                                                                             == server_id_1].sort_values('time_step')\n",
    "                            df_solution_server_id_2 = df_solution_server.loc[df_solution_server['server_id'] \n",
    "                                                                                             == server_id_2].sort_values('time_step')\n",
    "\n",
    "                            \n",
    "                            # remove calculations for server_id_1 and server_id_2\n",
    "                            cur_num_servers_1, cur_Z_1, cur_L_1, cur_C_1, cur_data_center_slots_1 = self.calculate_server_id(df_solution_server_id_1, append_to_solution=False)\n",
    "                            cur_num_servers_2, cur_Z_2, cur_L_2, cur_C_2, cur_data_center_slots_2 = self.calculate_server_id(df_solution_server_id_2, append_to_solution=False)\n",
    "\n",
    "                            df_solution_new_server_id = pd.concat([df_solution_server_id_1, df_solution_server_id_2]).sort_values('time_step')\n",
    "\n",
    "                            # drop dismiss of the server_id_1\n",
    "                            df_solution_new_server_id = df_solution_new_server_id[np.logical_not(np.logical_and(df_solution_new_server_id['action'] == 'dismiss',\n",
    "                                                                                                                df_solution_new_server_id['server_id'] == server_id_1))]\n",
    "                            \n",
    "                            # change buy of server_id_2 to move\n",
    "                            df_solution_new_server_id.loc[np.logical_and(df_solution_new_server_id['action'] == 'buy',\n",
    "                                                                        df_solution_new_server_id['server_id'] == server_id_2), 'action'] = 'move'\n",
    "                            \n",
    "                            df_solution_new_server_id['server_id'] = server_id_1\n",
    "\n",
    "                            cur_num_servers_new, cur_Z_new, cur_L_new, cur_C_new, cur_data_center_slots_new = self.calculate_server_id(df_solution_new_server_id, append_to_solution=False)\n",
    "\n",
    "                            new_data_center_slots = self.data_center_slots + cur_data_center_slots_new - cur_data_center_slots_1 - cur_data_center_slots_2 \n",
    "                            if np.any(self.data_center_max_slots < new_data_center_slots):\n",
    "                                continue\n",
    "\n",
    "                            new_num_servers = self.solution_num_servers + cur_num_servers_new - cur_num_servers_1 - cur_num_servers_2\n",
    "                            new_Z = self.solution_Z + cur_Z_new - cur_Z_1 - cur_Z_2\n",
    "                            new_L = (self.solution_L * self.solution_num_servers + cur_L_new * cur_num_servers_new - cur_L_1 * cur_num_servers_1 - cur_L_2 * cur_num_servers_2) / np.maximum(new_num_servers, 1)\n",
    "                            new_C = self.solution_C + cur_C_new - cur_C_1 - cur_C_2\n",
    "\n",
    "                            new_obj, (new_num_servers, new_Z, new_L, new_C, new_U, new_R) = self.calculate_new_obj(new_num_servers=new_num_servers,\n",
    "                                                                                                                new_Z=new_Z,\n",
    "                                                                                                                new_L=new_L,\n",
    "                                                                                                                new_C=new_C)\n",
    "\n",
    "                            if new_obj <= self.solution_obj:\n",
    "                                continue\n",
    "\n",
    "                            total_gain += new_obj - self.solution_obj\n",
    "                            n_merged += 1\n",
    "                            \n",
    "                            self.update_new_obj(new_obj=new_obj,\n",
    "                                                new_num_servers=new_num_servers,\n",
    "                                                new_Z=new_Z,\n",
    "                                                new_L=new_L,\n",
    "                                                new_C=new_C,\n",
    "                                                new_U=new_U,\n",
    "                                                new_R=new_R,\n",
    "                                                new_data_center_slots=new_data_center_slots)\n",
    "                            \n",
    "                            df_solution = df_solution[np.logical_not(np.logical_or(df_solution['server_id'] == server_id_1, \n",
    "                                                                                   df_solution['server_id'] == server_id_2))]\n",
    "                            df_solution = pd.concat([df_solution, df_solution_new_server_id])\n",
    "\n",
    "                            self.historical_server_ids.append({\n",
    "                                \"solution_action\": \"merge\",\n",
    "                                \"server_id\": server_id_1,\n",
    "                                \"action_string\": None,\n",
    "                                \"buy_time_step\": None,\n",
    "                                \"new_solution_obj\": new_obj,\n",
    "                                \"obj_gain\": new_obj - self.solution_obj,\n",
    "                                \"merge_with\": server_id_2,\n",
    "                            })\n",
    "                            \n",
    "                            removed_indexes_2.add(idx_1)\n",
    "                            removed_indexes_2.add(idx_2)\n",
    "                            changed = True\n",
    "                            break\n",
    "                    if not changed:\n",
    "                        break\n",
    "        logging.info(f\"Merged {n_merged} servers with total gain {total_gain}\")\n",
    "        \n",
    "        self.solution = df_solution.sort_values([\"server_id\", \"time_step\",]).to_dict('records')\n",
    "\n",
    "\n",
    "    def search_servers(self, df_single_server):\n",
    "        self.df_single_server = df_single_server\n",
    "\n",
    "        current_n_buys = len([action for action in self.solution if action['action'] == 'buy'])\n",
    "        cur_obj = self.solution_obj\n",
    "\n",
    "        for action_string in tqdm.tqdm(self.df_single_server['action_string'].values, desc=\"Searching actions\"):\n",
    "            if \"|buy-dismiss|\" in action_string:\n",
    "                self.search_buy_dismiss_combination(action_string)\n",
    "            elif \"|buy-move-dismiss|\" in action_string:\n",
    "                self.search_buy_move_dismiss_combination(action_string)\n",
    "\n",
    "        new_n_buys = len([action for action in self.solution if action['action'] == 'buy'])\n",
    "        new_obj = self.solution_obj\n",
    "\n",
    "        logging.info(f\"Added {new_n_buys - current_n_buys} servers with total gain {new_obj - cur_obj}\")\n",
    "\n",
    "    def search_buy_move_dismiss_combination(self, action_string):\n",
    "        action_comb = parse_action_string(action_string)\n",
    "\n",
    "        server_generation = action_comb[0]['server_generation']\n",
    "\n",
    "        datacenter_id_1 = action_comb[0]['datacenter_id']\n",
    "        datacenter_id_2 = action_comb[1]['datacenter_id']\n",
    "\n",
    "        move_age = action_comb[1]['move_age']\n",
    "        dissmiss_age = action_comb[2]['dismiss_age']\n",
    "\n",
    "\n",
    "        df_cur_server = self.df_servers_dict[server_generation]\n",
    "        df_datacenter_1 = self.df_datacenters_dict[datacenter_id_1]\n",
    "        df_datacenter_2 = self.df_datacenters_dict[datacenter_id_2]\n",
    "\n",
    "        utilization_threshold_1 = action_comb[0]['average_U']\n",
    "        utilization_threshold_2 = action_comb[1]['average_U']\n",
    "\n",
    "        sensitivity_1 = df_datacenter_1['latency_sensitivity']\n",
    "        sensitivity_2 = df_datacenter_2['latency_sensitivity']\n",
    "\n",
    "        datacenter_cost_of_energy_1 = df_datacenter_1['cost_of_energy']\n",
    "        datacenter_cost_of_energy_2 = df_datacenter_2['cost_of_energy']\n",
    "\n",
    "        server_capacity = df_cur_server['capacity']\n",
    "        server_energy_consumption = df_cur_server['energy_consumption']\n",
    "        server_life_expectancy = df_cur_server['life_expectancy']\n",
    "        server_average_maintenance_fee = df_cur_server['average_maintenance_fee']\n",
    "        server_purchase_price = df_cur_server['purchase_price']\n",
    "        server_cost_of_moving = df_cur_server['cost_of_moving']\n",
    "        server_slots_size = df_cur_server['slots_size']\n",
    "        server_release_time_start = df_cur_server['server_release_time_start']\n",
    "        server_release_time_end = df_cur_server['server_release_time_end']\n",
    "\n",
    "        datacenter_idx_1 = self.datacenter_ids_to_index[datacenter_id_1]\n",
    "        datacenter_idx_2 = self.datacenter_ids_to_index[datacenter_id_2]\n",
    "        server_idx = self.server_generation_to_idx[server_generation]\n",
    "        sensitivity_idx_1 = self.sensitivity_to_idx[sensitivity_1]\n",
    "        sensitivity_idx_2 = self.sensitivity_to_idx[sensitivity_2]\n",
    "\n",
    "        demand_arr_1 = self.actual_demand_by_server_generation[server_idx, :, self.sensitivity_to_idx[sensitivity_1]]\n",
    "        demand_arr_2 = self.actual_demand_by_server_generation[server_idx, :, self.sensitivity_to_idx[sensitivity_2]]\n",
    "        total_buys = 0\n",
    "        for buy_time_step in range(server_release_time_start, server_release_time_end + 1):\n",
    "            start_idx_1 = buy_time_step - 1\n",
    "            end_idx_1 = buy_time_step + move_age - 1\n",
    "            start_idx_2 = buy_time_step + move_age - 1\n",
    "            end_idx_2 = buy_time_step + dissmiss_age - 1\n",
    "\n",
    "            demand_subarr_1 = demand_arr_1[start_idx_1:end_idx_1]\n",
    "            demand_subarr_2 = demand_arr_2[start_idx_2:end_idx_2]\n",
    "            if demand_subarr_1.shape[0] < move_age or demand_subarr_1.shape[0] + demand_subarr_2.shape[0] < dissmiss_age:\n",
    "                break\n",
    "\n",
    "            while True:\n",
    "\n",
    "                # check utilization condition for datacenter 1\n",
    "                demand_subarr_1_capped = np.minimum(demand_subarr_1, server_capacity)\n",
    "                # if demand_subarr_1_capped[0] / server_capacity < utilization_threshold_1: # check the first time step\n",
    "                #     break\n",
    "\n",
    "                demand_subarr_1_capped = pad_array_to_multiple_of_12(demand_subarr_1_capped)\n",
    "                demand_subarr_1_capped = demand_subarr_1_capped.reshape(-1, 12)\n",
    "                average_utilization_1 = np.nanmean(demand_subarr_1_capped / server_capacity, axis=1)\n",
    "                if np.any(average_utilization_1 < utilization_threshold_1):\n",
    "                    break\n",
    "\n",
    "                # check utilization condition for datacenter 2\n",
    "                demand_subarr_2_capped = np.minimum(demand_subarr_2, server_capacity)\n",
    "                demand_subarr_2_capped = pad_array_to_multiple_of_12(demand_subarr_2_capped)\n",
    "                demand_subarr_2_capped = demand_subarr_2_capped.reshape(-1, 12)\n",
    "                average_utilization_2 = np.nanmean(demand_subarr_2_capped / server_capacity, axis=1)\n",
    "                if np.any(average_utilization_2 < utilization_threshold_2):\n",
    "                    break\n",
    "\n",
    "                # check data center slots\n",
    "                new_data_center_slots = np.copy(self.data_center_slots)\n",
    "                new_data_center_slots[datacenter_idx_1, start_idx_1:end_idx_1] += server_slots_size\n",
    "                new_data_center_slots[datacenter_idx_2, start_idx_2:end_idx_2] += server_slots_size\n",
    "                if np.any(self.data_center_max_slots < new_data_center_slots):\n",
    "                    break\n",
    "\n",
    "                # check if adding the server is beneficial by calculating the objective function\n",
    "\n",
    "                ### calculate for the new server\n",
    "                cur_Z = np.zeros(self.solution_Z.shape)\n",
    "                cur_Z[server_idx, start_idx_1:end_idx_1, sensitivity_idx_1] = server_capacity\n",
    "                cur_Z[server_idx, start_idx_2:end_idx_2, sensitivity_idx_2] = server_capacity\n",
    "\n",
    "                cur_L = np.zeros(self.solution_L.shape)\n",
    "                cur_L[start_idx_1:end_idx_2] = np.arange(1, dissmiss_age + 1) / server_life_expectancy\n",
    "\n",
    "                cur_E = np.zeros(self.solution_L.shape)\n",
    "                cur_E[start_idx_1:end_idx_2] = datacenter_cost_of_energy_1 * server_energy_consumption\n",
    "                cur_E[start_idx_2:end_idx_2] = datacenter_cost_of_energy_2 * server_energy_consumption\n",
    "\n",
    "                cur_alpha = np.zeros(self.solution_L.shape)\n",
    "                cur_alpha[start_idx_1:end_idx_2] = server_average_maintenance_fee \\\n",
    "                    * (1+1.5 * np.arange(1, dissmiss_age + 1) / server_life_expectancy * np.log2(1.5 * np.arange(1, dissmiss_age + 1) / server_life_expectancy))\n",
    "                cur_C = cur_E + cur_alpha\n",
    "                cur_C[start_idx_1] += server_purchase_price\n",
    "                cur_C[start_idx_2] += server_cost_of_moving\n",
    "\n",
    "                cur_num_servers = np.zeros(self.solution_L.shape)\n",
    "                cur_num_servers[start_idx_1:end_idx_2] = 1\n",
    "\n",
    "                new_num_servers, new_Z, new_L, new_C = self.calculate_new_obj_components(cur_num_servers=cur_num_servers,\n",
    "                                                                                        cur_Z=cur_Z,\n",
    "                                                                                        cur_L=cur_L,\n",
    "                                                                                        cur_C=cur_C)\n",
    "                new_obj, (new_num_servers, new_Z, new_L, new_C, new_U, new_R) = self.calculate_new_obj(new_num_servers=new_num_servers,\n",
    "                                                                                                    new_Z=new_Z,\n",
    "                                                                                                    new_L=new_L,\n",
    "                                                                                                    new_C=new_C)\n",
    "                if new_obj - self.solution_obj <= 0:\n",
    "                    break\n",
    "                new_server_id = self.generate_random_id()\n",
    "                \n",
    "                self.historical_server_ids.append({\n",
    "                    \"solution_action\": \"add\",\n",
    "                    \"server_id\": new_server_id,\n",
    "                    \"action_string\": action_string,\n",
    "                    \"buy_time_step\": buy_time_step,\n",
    "                    \"new_solution_obj\": new_obj,\n",
    "                    \"obj_gain\": new_obj - self.solution_obj,\n",
    "                    \"merge_with\": None,\n",
    "                })\n",
    "\n",
    "                self.update_new_obj(new_obj=new_obj,\n",
    "                                    new_num_servers=new_num_servers,\n",
    "                                    new_Z=new_Z,\n",
    "                                    new_L=new_L,\n",
    "                                    new_C=new_C,\n",
    "                                    new_U=new_U,\n",
    "                                    new_R=new_R,\n",
    "                                    new_data_center_slots=new_data_center_slots)\n",
    "\n",
    "                self.add_buy_move_dismiss_action(datacenter_id_1=datacenter_id_1,\n",
    "                                                    datacenter_id_2=datacenter_id_2,\n",
    "                                                    server_generation=server_generation,\n",
    "                                                    move_age=move_age,\n",
    "                                                    dismiss_age=dissmiss_age,\n",
    "                                                    time_step=buy_time_step,\n",
    "                                                    server_id=new_server_id)\n",
    "                total_buys += 1\n",
    "        # if self.verbose and total_buys > 0:\n",
    "        #     print(f\"Bought {total_buys} servers with action string {action_string}\")\n",
    "\n",
    "        \n",
    "    def search_buy_dismiss_combination(self, action_string, ):\n",
    "        action_comb = parse_action_string(action_string)\n",
    "\n",
    "        server_generation = action_comb[0]['server_generation']\n",
    "        datacenter_id = action_comb[0]['datacenter_id']\n",
    "\n",
    "        df_cur_server = self.df_servers_dict[server_generation]\n",
    "        df_datacenter = self.df_datacenters_dict[datacenter_id]\n",
    "\n",
    "        utilization_threshold = action_comb[0]['average_U']\n",
    "\n",
    "        sensitivity = df_datacenter['latency_sensitivity']\n",
    "        datacenter_cost_of_energy = df_datacenter['cost_of_energy']\n",
    "\n",
    "        server_capacity = df_cur_server['capacity']\n",
    "        server_energy_consumption = df_cur_server['energy_consumption']\n",
    "        server_life_expectancy = df_cur_server['life_expectancy']\n",
    "        server_average_maintenance_fee = df_cur_server['average_maintenance_fee']\n",
    "        server_purchase_price = df_cur_server['purchase_price']\n",
    "        server_slots_size = df_cur_server['slots_size']\n",
    "        server_release_time_start = df_cur_server['server_release_time_start']\n",
    "        server_release_time_end = df_cur_server['server_release_time_end']\n",
    "        dissmiss_age = action_comb[1]['dismiss_age']\n",
    "\n",
    "        datacenter_idx = self.datacenter_ids_to_index[datacenter_id]\n",
    "        server_idx = self.server_generation_to_idx[server_generation]\n",
    "        sensitivity_idx = self.sensitivity_to_idx[sensitivity]\n",
    "        demand_arr = self.actual_demand_by_server_generation[server_idx, :, sensitivity_idx]\n",
    "        total_buys = 0\n",
    "        for buy_time_step in range(server_release_time_start, server_release_time_end + 1):\n",
    "            start_idx = buy_time_step - 1\n",
    "            end_idx = buy_time_step + dissmiss_age - 1\n",
    "            demand_subarr = demand_arr[start_idx:end_idx]\n",
    "            if demand_subarr.shape[0] < dissmiss_age:\n",
    "                break\n",
    "\n",
    "            while True:\n",
    "                # check utilization condition\n",
    "                demand_subarr_capped = np.minimum(demand_subarr, server_capacity)\n",
    "\n",
    "                # if demand_subarr_capped[0] / server_capacity < utilization_threshold: # check the first time step\n",
    "                #     break\n",
    "                \n",
    "                demand_subarr_capped = pad_array_to_multiple_of_12(demand_subarr_capped)\n",
    "                demand_subarr_capped = demand_subarr_capped.reshape(-1, 12)\n",
    "                average_utilization = np.nanmean(demand_subarr_capped / server_capacity, axis=1)\n",
    "                if np.any(average_utilization < utilization_threshold):\n",
    "                    break\n",
    "\n",
    "                # check data center slots\n",
    "                new_data_center_slots = np.copy(self.data_center_slots)\n",
    "                new_data_center_slots[datacenter_idx, start_idx:end_idx] += server_slots_size\n",
    "\n",
    "                if np.any(self.data_center_max_slots < new_data_center_slots):\n",
    "                    break\n",
    "                \n",
    "                # check if adding the server is beneficial by calculating the objective function\n",
    "                cur_Z = np.zeros(self.solution_Z.shape)\n",
    "                cur_Z[server_idx, start_idx:end_idx, sensitivity_idx] = server_capacity\n",
    "                \n",
    "                cur_L = np.zeros((self.time_steps[1],))\n",
    "                cur_L[start_idx:end_idx] = np.arange(1, dissmiss_age + 1) / server_life_expectancy\n",
    "\n",
    "                cur_E = np.zeros((self.time_steps[1],))\n",
    "                cur_E[start_idx:end_idx] = datacenter_cost_of_energy * server_energy_consumption\n",
    "\n",
    "                cur_alpha = np.zeros((self.time_steps[1],))\n",
    "                cur_alpha[start_idx:end_idx] = server_average_maintenance_fee \\\n",
    "                    * (1+1.5 * np.arange(1, dissmiss_age + 1) / server_life_expectancy * np.log2(1.5 * np.arange(1, dissmiss_age + 1) / server_life_expectancy))\n",
    "                cur_C = cur_E + cur_alpha\n",
    "                cur_C[start_idx] += server_purchase_price\n",
    "\n",
    "                cur_num_servers = np.zeros((self.time_steps[1],))\n",
    "                cur_num_servers[start_idx:end_idx] = 1\n",
    "\n",
    "\n",
    "                new_num_servers, new_Z, new_L, new_C = self.calculate_new_obj_components(cur_num_servers=cur_num_servers,\n",
    "                                                                                        cur_Z=cur_Z,\n",
    "                                                                                        cur_L=cur_L,\n",
    "                                                                                        cur_C=cur_C)\n",
    "                new_obj, (new_num_servers, new_Z, new_L, new_C, new_U, new_R) = self.calculate_new_obj(new_num_servers=new_num_servers,\n",
    "                                                                                                       new_Z=new_Z,\n",
    "                                                                                                       new_L=new_L,\n",
    "                                                                                                       new_C=new_C)\n",
    "                \n",
    "                if new_obj - self.solution_obj <= 0:\n",
    "                    break\n",
    "\n",
    "                new_server_id = self.generate_random_id()\n",
    "                \n",
    "                self.historical_server_ids.append({\n",
    "                    \"solution_action\": \"add\",\n",
    "                    \"server_id\": new_server_id,\n",
    "                    \"action_string\": action_string,\n",
    "                    \"buy_time_step\": buy_time_step,\n",
    "                    \"new_solution_obj\": new_obj,\n",
    "                    \"obj_gain\": new_obj - self.solution_obj,\n",
    "                    \"merge_with\": None,\n",
    "                })\n",
    "                self.update_new_obj(new_obj=new_obj,\n",
    "                                    new_num_servers=new_num_servers,\n",
    "                                    new_Z=new_Z,\n",
    "                                    new_L=new_L,\n",
    "                                    new_C=new_C,\n",
    "                                    new_U=new_U,\n",
    "                                    new_R=new_R,\n",
    "                                    new_data_center_slots=new_data_center_slots)\n",
    "                \n",
    "                self.add_buy_dismiss_action(datacenter_id=datacenter_id, \n",
    "                                        server_generation=server_generation, \n",
    "                                        dismiss_age=dissmiss_age, \n",
    "                                        time_step=buy_time_step,\n",
    "                                        server_id=new_server_id)\n",
    "                total_buys += 1\n",
    "                    \n",
    "        # if self.verbose and total_buys > 0:\n",
    "        #     logging.info(f\"Bought {total_buys} servers with action string {action_string}\")\n",
    "\n",
    "    def add_buy_move_dismiss_action(self, datacenter_id_1, datacenter_id_2, server_generation, move_age, dismiss_age, time_step, server_id=None):\n",
    "        server_life_expectancy = self.df_servers_dict[server_generation]['life_expectancy']\n",
    "        if server_id is None:\n",
    "            server_id = self.generate_random_id()\n",
    "        self.solution.append({\n",
    "            \"time_step\": time_step,\n",
    "            \"action\": \"buy\",\n",
    "            \"datacenter_id\": datacenter_id_1,\n",
    "            \"server_generation\": server_generation,\n",
    "            \"server_id\": server_id\n",
    "        })\n",
    "\n",
    "        if time_step + move_age <= self.time_steps[1]:\n",
    "            self.solution.append({\n",
    "                \"time_step\": time_step + move_age,\n",
    "                \"action\": \"move\",\n",
    "                \"datacenter_id\": datacenter_id_2,\n",
    "                \"server_generation\": server_generation,\n",
    "                \"server_id\": server_id\n",
    "            })\n",
    "\n",
    "        if dismiss_age < server_life_expectancy and time_step + dismiss_age <= self.time_steps[1]:\n",
    "            self.solution.append({\n",
    "                \"time_step\": time_step + dismiss_age,\n",
    "                \"action\": \"dismiss\",\n",
    "                \"datacenter_id\": datacenter_id_2,\n",
    "                \"server_generation\": server_generation,\n",
    "                \"server_id\": server_id\n",
    "            })\n",
    "\n",
    "    def add_buy_dismiss_action(self, datacenter_id, server_generation, dismiss_age, time_step, server_id=None):\n",
    "        server_life_expectancy = self.df_servers_dict[server_generation]['life_expectancy']\n",
    "        \n",
    "        if server_id is None:\n",
    "            server_id = self.generate_random_id()\n",
    "        self.solution.append({\n",
    "            \"time_step\": time_step,\n",
    "            \"action\": \"buy\",\n",
    "            \"datacenter_id\": datacenter_id,\n",
    "            \"server_generation\": server_generation,\n",
    "            \"server_id\": server_id\n",
    "        })\n",
    "        if dismiss_age < server_life_expectancy and time_step + dismiss_age <= self.time_steps[1]:\n",
    "            self.solution.append({\n",
    "                \"time_step\": time_step + dismiss_age,\n",
    "                \"action\": \"dismiss\",\n",
    "                \"datacenter_id\": datacenter_id,\n",
    "                \"server_generation\": server_generation,\n",
    "                \"server_id\": server_id\n",
    "            })\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        data = {\n",
    "            \"solution\": self.solution,\n",
    "            \"solution_num_servers\": self.solution_num_servers,\n",
    "            \"solution_Z\": self.solution_Z,\n",
    "            \"solution_L\": self.solution_L,\n",
    "            \"solution_C\": self.solution_C,\n",
    "            \"datacenter_slots\": self.data_center_slots,\n",
    "            \"historical_server_ids\": self.historical_server_ids,\n",
    "            \"solution_selling_prices\": self.solution_selling_prices,\n",
    "        }\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def load_checkpoint(self, path, demand):\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.solution = data['solution']\n",
    "            cur_num_servers = data['solution_num_servers']\n",
    "            cur_Z = data['solution_Z']\n",
    "            cur_L = data['solution_L']\n",
    "            cur_C = data['solution_C']\n",
    "            cur_data_center_slots = data['datacenter_slots']\n",
    "            self.solution_selling_prices = data['solution_selling_prices']\n",
    "            if 'historical_server_ids' in data:\n",
    "                self.historical_server_ids = data['historical_server_ids']\n",
    "\n",
    "            self.load_demand(demand)\n",
    "\n",
    "            new_obj, (new_num_servers, new_Z, new_L, new_C, new_U, new_R) = self.calculate_new_obj(new_num_servers=cur_num_servers,\n",
    "                                                                                                new_Z=cur_Z,\n",
    "                                                                                                new_L=cur_L,\n",
    "                                                                                                new_C=cur_C)\n",
    "            self.update_new_obj(new_obj=new_obj,\n",
    "                                new_num_servers=new_num_servers,\n",
    "                                new_Z=new_Z,\n",
    "                                new_L=new_L,\n",
    "                                new_C=new_C,\n",
    "                                new_U=new_U,\n",
    "                                new_R=new_R,\n",
    "                                new_data_center_slots=cur_data_center_slots)\n",
    "    def solution_to_dataframe(self,):\n",
    "        df_fleet = pd.DataFrame(self.solution)\n",
    "        df_pricing_strategy = []\n",
    "        for time_step in range(1, self.time_steps[1]+1):\n",
    "            for server_generation in self.server_generation_to_idx.keys():\n",
    "                for sensitivity in self.sensitivity_to_idx.keys():\n",
    "                    df_pricing_strategy.append({\n",
    "                        \"time_step\": time_step,\n",
    "                        \"server_generation\": server_generation,\n",
    "                        \"latency_sensitivity\": sensitivity,\n",
    "                        \"price\": self.solution_selling_prices[self.server_generation_to_idx[server_generation], \n",
    "                                                              time_step-1, \n",
    "                                                              self.sensitivity_to_idx[sensitivity]]})\n",
    "        df_pricing_strategy = pd.DataFrame(df_pricing_strategy)\n",
    "        return df_fleet, df_pricing_strategy\n",
    "                    \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def solve_demand(demand, \n",
    "                 df_single_server, \n",
    "                 verbose=False, \n",
    "                 failure_rate_r=1.0, \n",
    "                 df_solution=None, \n",
    "                 output_file=None, \n",
    "                 restore_checkpoint_path=None, \n",
    "                 checkpoint_path=None, \n",
    "                 n_solving_loops=5):\n",
    "    _, datacenters, servers, selling_prices, elasticity = load_problem_data()\n",
    "    solver = Solver(df_servers=servers, \n",
    "                        df_data_centers=datacenters, \n",
    "                        df_selling_prices=selling_prices,\n",
    "                        df_price_elasticity_of_demand=elasticity,\n",
    "                        verbose=verbose)\n",
    "    solver.failure_rate = 0.07260491698699582 * failure_rate_r\n",
    "    if restore_checkpoint_path is not None and os.path.exists(restore_checkpoint_path):\n",
    "        solver.load_checkpoint(restore_checkpoint_path, demand=demand)\n",
    "    elif df_solution is not None:\n",
    "        solver.load_solution(df_solution, demand=demand)\n",
    "    else:\n",
    "        solver.init_solution(demand=demand)\n",
    "\n",
    "    best_obj = solver.solution_obj\n",
    "    logging.info(f\"Initial solution: {solver.solution_obj}\")\n",
    "\n",
    "    \n",
    "    def save_checkpoint():\n",
    "        if output_file is not None:\n",
    "            df_fleet, df_pricing_strategy = solver.solution_to_dataframe()    \n",
    "            save_solution(fleet=df_fleet,\n",
    "                          pricing_strategy=df_pricing_strategy,\n",
    "                          path=output_file)\n",
    "        if checkpoint_path is not None:\n",
    "            solver.save_checkpoint(checkpoint_path)\n",
    "\n",
    "    solver.remove_nonprofit_server_ids()\n",
    "    if solver.solution_obj > best_obj:\n",
    "        logging.info(f\"New best solution: {solver.solution_obj}\")\n",
    "        best_obj = solver.solution_obj\n",
    "        save_checkpoint()\n",
    "            \n",
    "    for _ in range(n_solving_loops):\n",
    "        for step in range(3):\n",
    "            if step == 0:\n",
    "                solver.search_servers(df_single_server=df_single_server)\n",
    "            elif step == 1:\n",
    "                solver.merge_server_ids(merge_gap_sizes=range(12))\n",
    "            elif step == 2:\n",
    "                solver.remove_nonprofit_server_ids()\n",
    "\n",
    "            if solver.solution_obj > best_obj:\n",
    "                logging.info(f\"New best solution: {solver.solution_obj}\")\n",
    "                best_obj = solver.solution_obj\n",
    "                save_checkpoint()\n",
    "    return solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solve seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_seed(seed):\n",
    "    OUTPUT_FOLDER = \"solution_v2\"\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "    output_file = f\"{OUTPUT_FOLDER}/{seed}.json\"\n",
    "    checkpoint_path = f\"{OUTPUT_FOLDER}/{seed}.pkl\"\n",
    "        \n",
    "    demand, datacenters, servers, selling_prices, elasticity = load_problem_data()\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    actual_demand = evaluation.get_actual_demand(demand)\n",
    "\n",
    "    df_single_server_buydismiss = pd.read_csv(\"resources/df_single_server_results_buydismiss_P.csv\")\n",
    "    df_single_server_buymovedismiss = pd.read_csv(\"resources/df_single_server_results_buymovedismiss_P.csv\")\n",
    "\n",
    "    list_df_single_server_buymovedismiss_filtered = []\n",
    "    for server_generataion in df_single_server_buymovedismiss['server_generation'].unique():\n",
    "        df_server_generation = df_single_server_buymovedismiss[df_single_server_buymovedismiss['server_generation'] == server_generataion]\n",
    "        df_server_generation = df_server_generation.iloc[:int(len(df_server_generation) * 0.2)]\n",
    "        list_df_single_server_buymovedismiss_filtered.append(df_server_generation)\n",
    "    df_single_server_buymovedismiss_filtered = pd.concat(list_df_single_server_buymovedismiss_filtered)\n",
    "    df_single_server_buymovedismiss_filtered = df_single_server_buymovedismiss_filtered.sort_values('score_per_slot', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    df_single_server = pd.concat([df_single_server_buydismiss, df_single_server_buymovedismiss_filtered]).iloc[:100]\n",
    "\n",
    "    solver = solve_demand(demand=actual_demand,\n",
    "                            df_single_server=df_single_server,\n",
    "                            verbose=True,\n",
    "                            failure_rate_r=1.0,\n",
    "                            n_solving_loops=10,\n",
    "                            output_file=output_file,\n",
    "                            restore_checkpoint_path=\"resources/base_solution.pkl\",\n",
    "                            checkpoint_path=checkpoint_path)\n",
    "    \n",
    "    return solver\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    seeds =  [2381, 5351, 6047, 6829, 9221, 9859, 8053, 1097, 8677, 2521]\n",
    "    \n",
    "    processes = []\n",
    "    for seed in seeds:\n",
    "        p = multiprocessing.Process(target=solve_seed, \n",
    "                                    args=(seed,), \n",
    "                                    name=f\"Solving Seed-{seed}\")\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
